{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixd Logit using PyBLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyblp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "# np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyblp.options.digits = 3\n",
    "pyblp.options.verbose = False\n",
    "pd.options.display.precision = 3\n",
    "pd.options.display.max_columns = 50\n",
    "# pd.options.display.float_format = '{:.5f}'.format\n",
    "\n",
    "import IPython.display\n",
    "IPython.display.display(IPython.display.HTML('<style>pre { white-space: pre !important; }</style>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will try to resolve the unrealistic issues in the pure logit model. There, we had unrealistic substitution effects. A good example on that is the blue bus/red bus paradox.\n",
    "\n",
    "To resolve this issue, we will try to add preference heterogeneity to the initial model. \n",
    "\n",
    "The model here is a random coefficient one:\n",
    "\n",
    "$$\n",
    "u_{ijt}=x_{jt}^\\prime\\beta_{it}+\\xi_{jt}+\\varepsilon_{ijt}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "u_{ijt} = \\overbrace{x_{jt}'}^{1\\times k}\\beta_{it} + \\zeta_{jt} + \\varepsilon_{ijt} \\\\\n",
    "    \\beta_{it} = \\beta + \\underbrace{\\Pi}_{k\\times d} \\underbrace{y_{it}}_{d\\times 1} + \\underbrace{\\Sigma}_{k\\times k} \\underbrace{\\nu_{it}}_{k\\times 1} \\,\\, ,\\,\\, \\nu_{it} \\sim N(0,I) \\rightarrow \\beta_{it} \\sim N(\\beta+\\Pi y_{it}, \\Sigma\\Sigma')\n",
    "\n",
    "$$\n",
    "\n",
    "Where, $y_{it}$ is just some demographic of agent $i$ in market $t$. $\\Pi$ shifts preferences for different characteristics based on observed demographics. Also, $\\Sigma$ shifts preferences according to “unobserved” preferences $\\nu_{it}$.\n",
    "\n",
    "Therefore, the parameter space here is $\\left(\\beta,\\Pi,\\Sigma\\right)$.\n",
    "\n",
    "\n",
    "So, the model is:\n",
    "\n",
    "$$\n",
    "u_{ijt}=\\underbrace{x_{jt}^{\\prime}\\beta+\\xi_{jt}}_{\\delta_{jt}}+\\underbrace{x_{jt}^{\\prime}(\\Sigma\\nu_{it}+\\Pi y_{it})}_{\\mu_{ijt}}+\\varepsilon_{ijt} \\\\\n",
    "\n",
    "s_{jt}=\\sum_{i\\in\\mathcal{I}_t}w_{it}\\cdot\\frac{\\exp[\\delta_{jt}+\\mu_{ijt}(\\Sigma,\\Pi)]}{1+\\sum_{k\\in\\mathcal{J}_t}\\exp[\\delta_{kt}+\\mu_{ikt}(\\Sigma,\\Pi)]}\\quad\\text{for all}\\quad j\\in\\mathcal{J}_t\n",
    "$$\n",
    "\n",
    "So, we have two data sets here. One is the product data, which is basically the same as before, the data on $j,t$. Yet, we need a new data on $i,t$. To create this dataset, we can do the following:\n",
    "\n",
    "1. Draw $|\\mathcal{I}_t| = 100$ from the agent data per market.\n",
    "2. Draw $\\nu_{it} \\sim N(0,I)$.\n",
    "3. Draw $y_{it}$ from the demographic data.\n",
    "4. Each type is equaly-likely. So, $w_{it} = \\frac{1}{|\\mathcal{I}_t|}$.\n",
    "\n",
    "\n",
    "Then, the solution is straightforward, using GMM-IV. First, using the market share above, for some guess on preference heterogeneity parameters, solve for the mean utilities. Then, using a GMM-IV regression, solve for $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic = pd.read_csv('https://raw.githubusercontent.com/Mixtape-Sessions/Demand-Estimation/main/Exercises/Data/demographics.csv')\n",
    "product = pd.read_csv('https://raw.githubusercontent.com/Mixtape-Sessions/Demand-Estimation/main/Exercises/Data/products.csv')\n",
    "\n",
    "demographic['log_income'] = np.log(demographic['quarterly_income'])\n",
    "\n",
    "product['demand_instruments0'] = product['price_instrument']\n",
    "product['market_size'] = product['city_population'] * 90\n",
    "product['market_share'] = product['servings_sold']/product['market_size']\n",
    "product['outside_share'] = 1 - product.groupby('market')['market_share'].transform('sum')\n",
    "\n",
    "demographic.rename(columns={'market': 'market_ids'}, inplace=True)\n",
    "product.rename(columns={'market': 'market_ids'}, inplace=True)\n",
    "product.rename(columns={'product': 'product_ids'}, inplace=True)\n",
    "product.rename(columns={'market_share': 'shares'}, inplace=True)\n",
    "product.rename(columns={'price_per_serving': 'prices'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The demographic dataset contains information about 20 individuals drawn from the Current Population Survey for each of the 94 markets in the product data. Each row is a different individual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Describe cross-market variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_ids</th>\n",
       "      <th>mushy_mean</th>\n",
       "      <th>mushy_std</th>\n",
       "      <th>prices_mean</th>\n",
       "      <th>prices_std</th>\n",
       "      <th>log_income_mean</th>\n",
       "      <th>log_income_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>94.0</td>\n",
       "      <td>94.000</td>\n",
       "      <td>94.000</td>\n",
       "      <td>94.000</td>\n",
       "      <td>94.000</td>\n",
       "      <td>94.000</td>\n",
       "      <td>94.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.029</td>\n",
       "      <td>8.091</td>\n",
       "      <td>0.885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.022</td>\n",
       "      <td>7.499</td>\n",
       "      <td>0.523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.026</td>\n",
       "      <td>7.872</td>\n",
       "      <td>0.710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.028</td>\n",
       "      <td>8.093</td>\n",
       "      <td>0.831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.031</td>\n",
       "      <td>8.338</td>\n",
       "      <td>1.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.038</td>\n",
       "      <td>8.622</td>\n",
       "      <td>1.539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       product_ids  mushy_mean  mushy_std  prices_mean  prices_std  \\\n",
       "count         94.0      94.000     94.000       94.000      94.000   \n",
       "mean          24.0       0.333      0.482        0.126       0.029   \n",
       "std            0.0       0.000      0.000        0.005       0.004   \n",
       "min           24.0       0.333      0.482        0.112       0.022   \n",
       "25%           24.0       0.333      0.482        0.122       0.026   \n",
       "50%           24.0       0.333      0.482        0.126       0.028   \n",
       "75%           24.0       0.333      0.482        0.129       0.031   \n",
       "max           24.0       0.333      0.482        0.138       0.038   \n",
       "\n",
       "       log_income_mean  log_income_std  \n",
       "count           94.000          94.000  \n",
       "mean             8.091           0.885  \n",
       "std              0.289           0.242  \n",
       "min              7.499           0.523  \n",
       "25%              7.872           0.710  \n",
       "50%              8.093           0.831  \n",
       "75%              8.338           1.086  \n",
       "max              8.622           1.539  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_number = product.groupby('market_ids').agg({'product_ids':\n",
    "    'count'})\n",
    "product_agg = product.groupby('market_ids')[['mushy', 'prices']].agg(['mean', 'std'])\n",
    "demographic_agg = demographic.groupby('market_ids')[['log_income']].agg(['mean', 'std'])\n",
    "\n",
    "# Flattening the columns in product_agg\n",
    "product_agg.columns = ['_'.join(col).strip() for col in product_agg.columns.values]\n",
    "# Flattening the columns in demographic_agg\n",
    "demographic_agg.columns = ['_'.join(col).strip() for col in demographic_agg.columns.values]\n",
    "\n",
    "merged = product_number.merge(product_agg, on='market_ids').merge(demographic_agg, on='market_ids')\n",
    "\n",
    "merged.describe().round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "As every market has 24 products, then, there is no variation in the mushyness and the number of products in this data set. Yet, there is cross-market variations in prices. Therefore, as for identifying preference heterogeneity, we may be able to identify those heterogeneities in preferences for prices (and not for mushyness or a constant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a good amount of cross-market demographic variation. Consumers' income varies a good amount across our markets. This means that using this variation, we have a hope of credibly estimating how income shifts the preference of different characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Estimate a simple model: Only Heterogeneity in Mushyness preferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, suppose that we only want to have heterogeneity in the coefficient of mushyness. Also, as there is not variation for mushyness between markets, we cannot add unobservable preference heterogeneity there. In other words, the model we want to estimate is:\n",
    "\n",
    "$$\n",
    "u_{ijt} = -\\alpha p_{jt} + \\beta M_{jt} + M_{jt}\\left(\\pi y_{it} + \\underbrace{\\sigma}_{= 0} \\nu_{it}\\right) + \\xi_{jt} + \\varepsilon_{ijt}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we only have three parameters $\\alpha,\\beta,\\pi$ to estimate. So, we need at least 3 moment conditions. the instruments we use for each of these three are:\n",
    "$(\\text{price instrument}, M_{jt},M_{jt} \\times \\bar{y}_t)$. Price instrument is already given in the product data, $M_{jt}$ is exogenous. Also, for $\\pi$, we can get mean income at each market and multiplied it by mushyness level of each product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use PyBLP, we need a product data and an agent data. The product data is already there. For the agent data, we will sample with replacement from the demographic data. Also, as for the unobservable terms in preference heterogeneity, although we will not need it in this part of the problem, we should sample from standard normal. This is done in the following code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating agent_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "random_seed = 42\n",
    "\n",
    "# Group by 'market_id' and sample with replacement\n",
    "agent_data = (\n",
    "    demographic\n",
    "    .groupby('market_ids', as_index=False)  # Group by 'market_id'\n",
    "    .apply(lambda x: x.sample(n=n_samples, replace=True, random_state=random_seed))  # Sample with replacement\n",
    "    .reset_index(drop=True)  # Reset index to get a clean DataFrame\n",
    ")\n",
    "\n",
    "agent_data['weights'] = 1 / n_samples\n",
    "\n",
    "rng = np.random.default_rng(random_seed)\n",
    "\n",
    "normal_samples = rng.normal(size=(len(agent_data), 1))\n",
    "\n",
    "agent_data['nodes0'] = normal_samples\n",
    "# Draw random samples from a standard normal distribution\n",
    "# normal_samples = rng.normal(size=(len(agent_data), 3))\n",
    "\n",
    "# # Create new columns in agent_data\n",
    "# agent_data[['nodes0', 'nodes1', 'nodes2']] = normal_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>market_ids</th>\n",
       "      <th>quarterly_income</th>\n",
       "      <th>log_income</th>\n",
       "      <th>weights</th>\n",
       "      <th>nodes0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18625</th>\n",
       "      <td>C14Q1</td>\n",
       "      <td>7360.966</td>\n",
       "      <td>8.904</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42611</th>\n",
       "      <td>C31Q1</td>\n",
       "      <td>6383.453</td>\n",
       "      <td>8.761</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77885</th>\n",
       "      <td>C52Q2</td>\n",
       "      <td>2552.445</td>\n",
       "      <td>7.845</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30309</th>\n",
       "      <td>C24Q1</td>\n",
       "      <td>6474.351</td>\n",
       "      <td>8.776</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-1.495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7519</th>\n",
       "      <td>C05Q2</td>\n",
       "      <td>4900.165</td>\n",
       "      <td>8.497</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-1.503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      market_ids  quarterly_income  log_income  weights  nodes0\n",
       "18625      C14Q1          7360.966       8.904    0.001   0.366\n",
       "42611      C31Q1          6383.453       8.761    0.001  -0.598\n",
       "77885      C52Q2          2552.445       7.845    0.001   1.249\n",
       "30309      C24Q1          6474.351       8.776    0.001  -1.495\n",
       "7519       C05Q2          4900.165       8.497    0.001  -1.503"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_data.sample(5, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have the weights for each individual, the integral for the market shares can be rewritten as follows:\n",
    "\n",
    "$$\n",
    "s_{jt}(\\delta_{jt},\\Sigma,\\Pi)=\\sum_{i\\in\\mathcal{I}_t}w_{it}\\cdot\\frac{\\exp[\\delta_{jt}+\\mu_{ijt}(\\Sigma,\\Pi)]}{1+\\sum_{k\\in\\mathcal{J}_t}\\exp[\\delta_{kt}+\\mu_{ikt}(\\Sigma,\\Pi)]}\\quad\\text{for all}\\quad j\\in\\mathcal{J}_t\n",
    "$$\n",
    "\n",
    "So, given the market shares and a guess on heterogeneity parameters, we can inverse this function to find:\n",
    "\n",
    "$$\n",
    "\\delta_{jt}(S_{t},\\Sigma,\\Pi) = x_{jt}^\\prime\\beta+\\xi_{jt}\n",
    "$$\n",
    "Then, using GMM-IV, we can solve for $\\beta(\\Sigma,\\Pi)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the other instrument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = product.merge(\n",
    "    pd.DataFrame(demographic.groupby('market_ids')['log_income'].mean()).rename(columns={'log_income':\n",
    "        'log_income_mean'}),\n",
    "        on='market_ids')\n",
    "product['demand_instruments1'] = product['log_income_mean'] * product['mushy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to give the formulation to PyBLP. First, we will give linear parts (which are prices and the absorbed fixed effects) and then, the nonlinear part (which is the mushyness)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_formulations = (pyblp.Formulation('0 + prices', \n",
    "            absorb='C(market_ids) + C(product_ids)'), pyblp.Formulation('0 + mushy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also an agent formulation, which defines the demographics we have for each market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_formulation = pyblp.Formulation('0 + log_income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dimensions:\n",
       "=============================================\n",
       " T    N      I     K1    K2    D    MD    ED \n",
       "---  ----  -----  ----  ----  ---  ----  ----\n",
       "94   2256  94000   1     1     1    2     2  \n",
       "=============================================\n",
       "\n",
       "Formulations:\n",
       "=========================================\n",
       "       Column Indices:             0     \n",
       "-----------------------------  ----------\n",
       " X1: Linear Characteristics      prices  \n",
       "X2: Nonlinear Characteristics    mushy   \n",
       "       d: Demographics         log_income\n",
       "========================================="
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mushy_problem  = pyblp.Problem(product_formulations, product, agent_formulation, agent_data)\n",
    "mushy_problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the model has 94 markets, 2256 product-market pair, 94000 agents (1000 agent for each market, which we sampled with replacement from demographic data). Also, $K_1$ is the number of linear product characteristics, $K_2$ is the number of nonlinear product characteristics, $D$ is the number of demographic features, $MD$ is the number of demand side instruments (Which is 2), and $ED$ is number of absorbed fixed effects of the demand side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization = pyblp.Optimization('trust-constr', {'gtol': 1e-8, 'xtol': 1e-8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, in PyBLP, whenever we set initial value of some parameter to 0, it just fix it to zero and does not try to solve for it. Also, as explained before, the only parameters we need to solve in GMM are just nonlinear ones, as the linear ones are just functions of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Problem Results Summary:\n",
       "===================================================================================\n",
       "GMM   Objective  Gradient              Clipped  Weighting Matrix  Covariance Matrix\n",
       "Step    Value      Norm      Hessian   Shares   Condition Number  Condition Number \n",
       "----  ---------  ---------  ---------  -------  ----------------  -----------------\n",
       " 1    +2.86E-20  +1.69E-09  +4.99E+01     0        +4.39E+01          +3.48E+01    \n",
       "===================================================================================\n",
       "\n",
       "Cumulative Statistics:\n",
       "===========================================================================\n",
       "Computation  Optimizer  Optimization   Objective   Fixed Point  Contraction\n",
       "   Time      Converged   Iterations   Evaluations  Iterations   Evaluations\n",
       "-----------  ---------  ------------  -----------  -----------  -----------\n",
       " 00:00:14       Yes          9            10          4661         15036   \n",
       "===========================================================================\n",
       "\n",
       "Nonlinear Coefficient Estimates (Robust SEs in Parentheses):\n",
       "========================================\n",
       "Sigma:    mushy    |   Pi:   log_income \n",
       "------  ---------  |  -----  -----------\n",
       "mushy   +0.00E+00  |  mushy   +2.57E-01 \n",
       "                   |         (+1.64E-01)\n",
       "========================================\n",
       "\n",
       "Beta Estimates (Robust SEs in Parentheses):\n",
       "===========\n",
       "  prices   \n",
       "-----------\n",
       " -3.06E+01 \n",
       "(+9.66E-01)\n",
       "==========="
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mushy_results = mushy_problem.solve(sigma=0, pi=1, method='1s', optimization=optimization)\n",
    "mushy_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of moment conditions is equal to the number of paramters, the model is just identified and the objective of GMM converges to 0. Also, the hessian is positive, meaning that it is indeed satisfying the SOC.\n",
    "\n",
    "Notice, as the estimate for $\\pi$ is positive, it means that, higher income agents prefer mushyness more. Also, the estimate of parameter of price is $-30.6$ which is the same as the pure logit case with fixed effects and IV (as should be as we have no preference heterogeneity for the prices in the current model).\n",
    "\n",
    "Moreover, $\\frac{\\pi}{\\alpha} = \\frac{0.257}{30.6} = 0.0084$. In other words, with a one percent increase in income (as income is in log terms here), willingness to pay for mushyness increases by $0.0084$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make sure you get the same estimate with random starting values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_pi: 0.9763, pi_estimate: 0.2567\n",
      "initial_pi: -1.6596, pi_estimate: 0.2567\n",
      "initial_pi: -1.2801, pi_estimate: 0.2567\n"
     ]
    }
   ],
   "source": [
    "n_seeds = 3\n",
    "\n",
    "pi_bounds = (-10,10)\n",
    "for seed in range(n_seeds):\n",
    "    np.random.seed(seed)\n",
    "    initial_pi = np.random.uniform(*pi_bounds)\n",
    "    result = mushy_problem.solve(sigma=0, pi=initial_pi, method='1s', optimization=optimization)\n",
    "    print(f'initial_pi: {initial_pi:.4f}, pi_estimate: {result.pi[0][0]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we get the same estimate for any initial value of pi. This validates our estimate. If you have a more complicated model with many parameters and many instruments, you may often get a global minimum, and sometimes get a local minimum. Optimizers aren't perfect, and sometimes terminate prematurely, even with tight termination conditions. You should select the global one for your final estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate changes to the price cut counterfactual"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
