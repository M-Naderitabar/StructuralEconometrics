{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "from numpy.linalg import matrix_power\n",
    "from scipy.integrate import quad\n",
    "from scipy.integrate import dblquad\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.polynomial.hermite import hermgauss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PS0 Empirical IO: Numerical Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Logit Inclusive Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Convexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logit inclusive value is the log of the denominator of a logit probability of choice. It is as follows:\n",
    "\n",
    "$$\n",
    "IV = \\log \\sum_{i=0}^N \\exp(x_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $x_0 = 0$. Then, $IV(x_1, ... , x_n) = \\log \\sum_{i=1}^N (1+\\exp(x_i))$. We want to show that this function is convex everywhere. The second derivative of this function is:\n",
    "$$\n",
    "\\frac{\\partial^2}{\\partial x_i^2} \\log\\left(N + \\sum_{j=1}^N \\exp(x_j)\\right) = \\exp(2x_i) \\left( \\frac{S - 1}{S^2} \\right)\n",
    " \\\\\n",
    "S = N + \\sum_{j=1}^N \\exp(x_j).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is indeed positive. Therefore, this function is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Large $x_i$ problem\n",
    "\n",
    "For large values of $x_i$, we cannot manage to find the exponential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To resolve this issue, we now that, given $m = \\max_i x_i$:\n",
    "\n",
    "$$\n",
    "IV = \\log \\sum_{i=0}^N \\exp(x_i) = \\log \\left(\\exp(m)\\sum_{i=0}^N \\exp(x_i - m)\\right)\n",
    "$$\n",
    "Therefore, we have:\n",
    "\n",
    "$$\n",
    "IV = m + \\log \\sum_{i=0}^N \\exp(x_i - m)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_IV(x):\n",
    "    m = np.max(x)\n",
    "    IV = m + np.log(np.sum(np.exp(x - m)))\n",
    "    return IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the above method, IV is: 800.3132616875182\n",
      "Using the naive method, IV is: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\AppData\\Local\\Temp\\ipykernel_24228\\1108397953.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  IV_unstable = np.log(np.sum(np.exp(x)))\n"
     ]
    }
   ],
   "source": [
    "x = np.array([300, 400, 500, 750, 799, 800])\n",
    "IV_stable = solve_IV(x)\n",
    "print(\"Using the above method, IV is:\", IV_stable)\n",
    "IV_unstable = np.log(np.sum(np.exp(x)))\n",
    "print(\"Using the naive method, IV is:\", IV_unstable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. logsumexp function in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using scipy's logsumexp, IV is: 800.3132616875182\n"
     ]
    }
   ],
   "source": [
    "IV_stable_python = logsumexp(x)\n",
    "print(\"Using scipy's logsumexp, IV is:\", IV_stable_python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function in python uses the same method as explained above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Markov Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the goal is to write a function to solve for the ergodic distribution of a markov process, given its transition matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this properly, we can use the idea of eigenvalues and eigen vectors. We know that, the ergodic distribution is the fixed point of $\\pi P$. Therefore:\n",
    "\n",
    "$$\n",
    "\\pi = \\pi P \\rightarrow (P' - I)\\pi' = 0\n",
    "$$\n",
    "\n",
    "So, the ergodic distribution is the normalized eigenvector (should be added to one) corresponding to eigenvalue of 1 for matrix $P'$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ergodic_distribution(P):\n",
    "    # Step 1: Compute the eigenvalues and eigenvectors of the transpose of P\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(P.T)\n",
    "    \n",
    "    # Step 2: Find the eigenvector corresponding to the eigenvalue 1\n",
    "    # We assume there is exactly one eigenvalue that is 1 for a valid transition matrix\n",
    "    eigenvector = eigenvectors[:, np.isclose(eigenvalues, 1)]\n",
    "    \n",
    "    # Step 3: Normalize the eigenvector\n",
    "    steady_state = eigenvector[:, 0]  # Extract the eigenvector (as it's returned in a 2D array)\n",
    "    steady_state = steady_state / np.sum(steady_state)\n",
    "    \n",
    "    # Ensure the result is real (numerical computations may result in a complex vector with very small imaginary parts)\n",
    "    steady_state = steady_state.real\n",
    "    \n",
    "    return steady_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The steady state distribution is: [0.31  0.241 0.448]\n"
     ]
    }
   ],
   "source": [
    "P = np.array([[0.2, 0.4, 0.4],\n",
    "              [0.1, 0.3, 0.6],\n",
    "              [0.5, 0.1, 0.4]])\n",
    "steady_state = ergodic_distribution(P)\n",
    "print(\"The steady state distribution is:\", steady_state.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, notice that:\n",
    "$$\n",
    "\\pi_t = \\pi_0 P^t\n",
    "$$\n",
    "So, to find the ergodic distribution, we can set $\\pi_0 = [1, 0, ..., 0]$ and find $\\pi_t$ for large values of $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ergodic distribution is: [0.31  0.241 0.448]\n"
     ]
    }
   ],
   "source": [
    "pi_0 = np.array([1, 0, 0])\n",
    "ergodic = pi_0 @ matrix_power(P, 100)\n",
    "print(\"The ergodic distribution is:\", ergodic.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see that these are the same results. When reached to the ergodic distribution, the distribution of transitioning from each alternative is the same and equal to the ergodic distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Numerical Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the goal is to solve for the logit choice probability by numerical integration. \n",
    "\n",
    "$$\n",
    "p(X, \\theta)=\\int_{-\\infty}^{\\infty} \\frac{\\exp \\left(\\beta_i X\\right)}{1+\\exp \\left(\\beta_i X\\right)} f\\left(\\beta_i \\mid \\theta\\right) \\partial \\beta_i \\,\\, ,\\,\\, f\\sim N(0.5,2) \\,\\, ,\\,\\, X = 0.5\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the quad function in python, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The integral is: 0.555939162843465\n"
     ]
    }
   ],
   "source": [
    "X = 0.5\n",
    "mean = 0.5\n",
    "sigma = np.sqrt(2)\n",
    "integral_true = quad(lambda beta: np.exp(beta*X)/(1 + np.exp(beta*X))\n",
    "                * norm.pdf(beta, mean, sigma), -20, 20, epsabs=1e-14)[0]\n",
    "print(\"The integral is:\", integral_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximation using Monte Carlo method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method is as follows (for 1D integral):\n",
    "\n",
    "$$\n",
    "I = \\int_a^b f(x) \\, dx \\rightarrow \\hat{I} = (b-a) \\frac{1}{N} \\sum_{i=1}^N f(x_i)\n",
    "$$\n",
    "where, $x_i$ s are uniformly randomly drawn in $(a,b)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binomiallogit(beta, params):\n",
    "    X = params[0]\n",
    "    mean = params[1]\n",
    "    sigma = params[2]\n",
    "    return np.exp(beta*X)/(1 + np.exp(beta*X)) * norm.pdf(beta, mean, sigma)\n",
    "    \n",
    "def monte_carlo_integration(func, up_lim, low_lim,\n",
    "                            params, n_samples=1000):\n",
    "    samples = np.random.uniform(low_lim, up_lim, size=n_samples)\n",
    "    # samples = np.linspace(low_lim, up_lim, n_samples)\n",
    "    integral = (up_lim - low_lim) * np.mean(func(samples, params))\n",
    "    return integral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The integral using 20 samples is: 0.6915976993037517\n",
      "The integral using 400 samples is: 0.5457227383948215\n"
     ]
    }
   ],
   "source": [
    "params = [X, mean, sigma]\n",
    "iter = 10\n",
    "integral_MC20 = 0\n",
    "integral_MC400 = 0\n",
    "for i in range(iter):\n",
    "    integral_MC20 += monte_carlo_integration(binomiallogit, 10, -10, params, n_samples=20)\n",
    "integral_MC20 /= iter\n",
    "for i in range(iter):\n",
    "    integral_MC400 += monte_carlo_integration(binomiallogit, 10, -10, params, n_samples=400)\n",
    "integral_MC400 /= iter\n",
    "\n",
    "print(\"The integral using 20 samples is:\", integral_MC20)\n",
    "print(\"The integral using 400 samples is:\", integral_MC400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximation using Gauss-Hermite Quadrature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GM is used for when the goal is to solve for some integral of the following form, with the following approximation and weights.\n",
    "\n",
    "$$\n",
    "\\int_{-\\infty}^{\\infty} e^{-x^2} f(x) \\, dx \\approx \\sum_{i=1}^{n} w_i f(x_i)\n",
    "\\,\\, ,\\,\\, w_i = \\frac{2^{n-1} n! \\sqrt{\\pi}}{n^2 \\left[ H_{n-1}(x_i) \\right]^2}\n",
    "$$\n",
    "\n",
    "So, as in our case, the integral is as follows:\n",
    "\n",
    "$$\n",
    "I = \\int_{-\\infty}^{\\infty} \\frac{\\exp \\left(\\beta_i X\\right)}{1+\\exp \\left(\\beta_i X\\right)} \n",
    "\\frac{1}{\\sqrt{2\\pi} \\sigma} \\exp\\left(-\\left(\\frac{\\beta_i - \\mu}{\\sqrt{2}\\sigma}\\right)^2\\right)\\partial \\beta_i\n",
    "$$\n",
    "Then, define $y = \\frac{\\beta_i - \\mu}{\\sqrt{2} \\sigma}$, then:\n",
    "\n",
    "$$\n",
    "I = \\frac{1}{\\sqrt{\\pi}} \\int_{-\\infty}^\\infty \\frac{\\exp \\left((\\sqrt{2} \\sigma y + \\mu) X\\right)}{1+\\exp \\left((\\sqrt{2} \\sigma y + \\mu) X\\right)} e^{-y^2} \\, dy\n",
    "$$\n",
    "So, using the GH formula, we have (define $g(y) = \\frac{\\exp \\left((\\sqrt{2} \\sigma y + \\mu) X\\right)}{1+\\exp \\left((\\sqrt{2} \\sigma y + \\mu) X\\right)} $)\n",
    "$$\n",
    "I \\approx \\frac{1}{\\sqrt{\\pi}} \\sum_{i=1}^{n} w_i g(x_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "def G_func(roots, params):\n",
    "    X = params[0]\n",
    "    mean = params[1]\n",
    "    sigma = params[2]\n",
    "    beta = np.sqrt(2) * sigma * roots + mean\n",
    "    g = np.exp(beta*X)/(1 + np.exp(beta*X))\n",
    "    return g\n",
    "def Gauss_Hermite_integral(n, params):\n",
    "    roots, weights = hermgauss(n)\n",
    "    g = G_func(roots, params)\n",
    "    return np.sum(weights * g) / np.sqrt(np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The integral using Gauss-Hermite quadrature is: 0.5559391582008274\n"
     ]
    }
   ],
   "source": [
    "integral_GH = Gauss_Hermite_integral(10, params)\n",
    "print(\"The integral using Gauss-Hermite quadrature is:\", integral_GH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for summary, here are the results for different methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Value: 0.555939162843465\n",
      "Approximation using Monte Carlo method (20 samples): 0.6915976993037517\n",
      "Approximation using Monte Carlo method (400 samples): 0.5457227383948215\n",
      "Approximation using Gauss-Hermite Quadrature: 0.5559391582008274\n"
     ]
    }
   ],
   "source": [
    "print(\"True Value:\", integral_true)\n",
    "print(\"Approximation using Monte Carlo method (20 samples):\", integral_MC20)\n",
    "print(\"Approximation using Monte Carlo method (400 samples):\", integral_MC400)\n",
    "print(\"Approximation using Gauss-Hermite Quadrature:\", integral_GH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two dimension case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose that:\n",
    "\n",
    "$$\n",
    "f \\sim N\\left((0.5,1) , \\begin{pmatrix}\n",
    "4 & 0 \\\\ 0 & 1\n",
    "\\end{pmatrix}\\right) \\,\\, ,\\,\\, X = (0.5, 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_normal_pdf(x, mean, cov):\n",
    "    result = np.zeros((x.shape[0]))\n",
    "    for i in range(x.shape[0]):\n",
    "        result[i] = multivariate_normal.pdf(x[i], mean=mean, cov=cov)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### True Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The true integral is: 0.7144838053940904\n"
     ]
    }
   ],
   "source": [
    "X = np.array([0.5,1])\n",
    "mu = np.array([0.5,1])\n",
    "var = np.array([[4,0],[0,1]])\n",
    "\n",
    "integral_true_2D = dblquad(lambda beta_1, beta_2: \n",
    "    np.exp(np.dot(np.array([beta_1,beta_2]),X))/(1 + np.exp(\n",
    "        np.dot(np.array([beta_1,beta_2]),X)))* multivariate_normal_pdf(\n",
    "            np.array([[beta_1,beta_2]]), mean=mu, cov=var), \n",
    "        -20, 20, -20, 20, epsabs=1e-14)[0]\n",
    "print(\"The true integral is:\", integral_true_2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approximation using Monte Carlo method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above. Though, we should multiply the average by the volume of the integraion space. In other words:\n",
    "\n",
    "$$\n",
    "I = \\int_{\\mathbb{V}} f(\\boldsymbol{x}) \\,d\\boldsymbol{x} \\rightarrow \\hat{I}_{MC_N} = \\text{Volume}(\\mathbb{V}) \\times \\frac{1}{N} \\sum_{i=1}^N f(\\boldsymbol{x}_i)\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binomiallogit2D(beta_1, beta_2, params):\n",
    "    X = params[0]\n",
    "    mean = params[1]\n",
    "    sigma = params[2]\n",
    "    return np.exp(np.array([beta_1, beta_2]).T @ X)/(1 +\n",
    "                                   np.exp(np.array([beta_1,beta_2]).T @ X)\n",
    "                                   ) * multivariate_normal_pdf(\n",
    "                                       np.array([beta_1,beta_2]).T, mean, sigma\n",
    "                                       )\n",
    "    \n",
    "def monte_carlo_integration2D(func, low_lim, up_lim,\n",
    "                            params, n_samples=1000):\n",
    "    beta_1 = np.random.uniform(low_lim, up_lim, size=n_samples)\n",
    "    beta_2 = np.random.uniform(low_lim, up_lim, size=n_samples)\n",
    "    integral = (up_lim - low_lim)**2 * np.mean(func(beta_1, beta_2, params))\n",
    "    return integral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The integral using 20 samples is: 0.6318587213389532\n",
      "The integral using 400 samples is: 0.6959738211991421\n"
     ]
    }
   ],
   "source": [
    "iter = 10\n",
    "integral_MC202D = 0\n",
    "integral_MC4002D = 0\n",
    "for i in range(iter):\n",
    "    integral_MC202D += monte_carlo_integration2D(binomiallogit2D, -10, 10, [X, mu, var], n_samples=20)\n",
    "integral_MC202D /= iter\n",
    "for i in range(iter):\n",
    "    integral_MC4002D += monte_carlo_integration2D(binomiallogit2D, -10, 10, [X, mu, var], n_samples=400)\n",
    "integral_MC4002D /= iter\n",
    "\n",
    "print(\"The integral using 20 samples is:\", integral_MC202D)\n",
    "print(\"The integral using 400 samples is:\", integral_MC4002D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximation using Gauss-Hermite Quadrature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 2D case, we can use the following idea:\n",
    "\n",
    "$$\n",
    "\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} e^{-(x^2 + y^2)} f(x, y) \\, dx \\, dy \\approx \\sum_{i=1}^n \\sum_{j=1}^n w_i w_j f(x_i, y_j)\n",
    "$$\n",
    "\n",
    "For our case, the integral is as follows:\n",
    "\n",
    "$$\n",
    "I = \\frac{1}{2\\pi\\sigma_1\\sigma_2}\\int_{\\beta_1}\\int_{\\beta_2} \n",
    "\\frac{\\exp(\\beta'X)}{1+\\exp{\\beta'X}} \\exp{\\left\\{-\\left(\\left(\\frac{\\beta_1 - \\mu_1}{\\sqrt{2}\\sigma_1}\\right)^2 + \\left(\\frac{\\beta_2 - \\mu_2}{\\sqrt{2}\\sigma_2}\\right)^2\\right)\\right\\}}\\,d\\beta_1\\,d\\beta_2\n",
    "$$\n",
    "\n",
    "So, defining, $y_1 = \\frac{\\beta_1 - \\mu_1}{\\sqrt{2}\\sigma_1} , y_2 = \\frac{\\beta_2 - \\mu_2}{\\sqrt{2}\\sigma_2}$, we have:\n",
    "$$\n",
    "I = \\frac{1}{\\pi} \\int\\int \\frac{\\exp((\\sqrt{2}\\sigma_1 y_1 + \\mu_1) X_1 + (\\sqrt{2}\\sigma_2 y_2 + \\mu_2) X_2)}{1+\\exp{(\\sqrt{2}\\sigma_1 y_1 + \\mu_1) X_1 + (\\sqrt{2}\\sigma_2 y_2 + \\mu_2) X_2)}} e^{-(y_1^2 + y_2^2)} \\,dy_1 \\, dy_2\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\n",
    "\\hat{I}_{GH_N} = \\frac{1}{\\pi} \\sum_{i=1}^n \\sum_{j=1}^n w_i w_j g(y_{1,i}, y_{2,j})\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_func_2D(roots, params, n):\n",
    "    X = params[0]\n",
    "    mean = params[1]\n",
    "    sigma = params[2]\n",
    "    beta = roots * (np.sqrt(2) * sigma) + mean\n",
    "    g = np.exp(beta @ X)/(1 + np.exp(beta @ X))\n",
    "    g = g.reshape([n, n])\n",
    "    return g\n",
    "\n",
    "def Gauss_Hermite_integral_2D(n, params):\n",
    "    roots, weights = hermgauss(n)\n",
    "    Y_1, Y_2 = np.meshgrid(roots, roots)\n",
    "    W_1, W_2 = np.meshgrid(weights, weights)\n",
    "    roots = np.array([Y_1.flatten(), Y_2.flatten()]).T\n",
    "    g = G_func_2D(roots, params, n)\n",
    "    result = np.sum(W_1 * W_2 * g) / np.pi\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The integral using Gauss-Hermite quadrature is: 0.7144838072089089\n"
     ]
    }
   ],
   "source": [
    "sigma = np.array([np.sqrt(var[0,0]), np.sqrt(var[1,1])])\n",
    "integral_GH_2D = Gauss_Hermite_integral_2D(10, [X, mu, sigma])\n",
    "print(\"The integral using Gauss-Hermite quadrature is:\", integral_GH_2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Value: 0.7144838053940904\n",
      "Approximation using Monte Carlo method (20 samples): 0.6318587213389532\n",
      "Approximation using Monte Carlo method (400 samples): 0.6959738211991421\n",
      "Approximation using Gauss-Hermite Quadrature: 0.7144838072089089\n"
     ]
    }
   ],
   "source": [
    "print(\"True Value:\", integral_true_2D)\n",
    "print(\"Approximation using Monte Carlo method (20 samples):\", integral_MC202D)\n",
    "print(\"Approximation using Monte Carlo method (400 samples):\", integral_MC4002D)\n",
    "print(\"Approximation using Gauss-Hermite Quadrature:\", integral_GH_2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can infer how Gauss-Hermite is the most efficient way to do this integrals numerically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
